{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f615b0d",
   "metadata": {},
   "source": [
    "## RNN Basics\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are made for handling **sequences** — like text, time series, or signals. Unlike regular networks, they have memory. They remember what came before, so they can use it to influence the next output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ba36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb188f1",
   "metadata": {},
   "source": [
    "### The Problem: Sequence Prediction\n",
    "\n",
    "Let’s say we have this repeating pattern:\n",
    "\n",
    "`[0, 1, 0, 1, 0, 1, ...]`\n",
    "\n",
    "We want the RNN to learn this pattern — that every time it sees a `0`, it should predict `1`, and every time it sees `1`, it should predict `0`.\n",
    "\n",
    "We’ll feed one number at a time, and the RNN should learn to \"remember\" the previous value to guess what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7eca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [0, 1] * 20; inputs, targets = sequence[:-1], sequence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5742c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f21577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, lr=0.1, epochs=100):\n",
    "        \n",
    "        self.w_input_hidden = random.uniform(-1, 1)  # input to hidden\n",
    "        self.w_hidden_hidden = random.uniform(-1, 1)  # hidden to hidden (recurrent)\n",
    "        self.w_hidden_output = random.uniform(-1, 1)  # hidden to output\n",
    "        self.b_hidden = random.uniform(-1, 1)\n",
    "        self.b_output = random.uniform(-1, 1)\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        for epoch in range(self.epochs):\n",
    "            hidden_state = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for x, target in zip(inputs, targets):\n",
    "                hidden_input = self.w_input_hidden * x + self.w_hidden_hidden * hidden_state + self.b_hidden\n",
    "                hidden_state_new = sigmoid(hidden_input)\n",
    "\n",
    "                output_input = self.w_hidden_output * hidden_state_new + self.b_output\n",
    "                prediction = sigmoid(output_input)\n",
    "\n",
    "                error = prediction - target\n",
    "                loss = error ** 2\n",
    "                total_loss += loss\n",
    "\n",
    "                d_output = error * sigmoid_derivative(prediction)\n",
    "                d_hidden = d_output * self.w_hidden_output * sigmoid_derivative(hidden_state_new)\n",
    "\n",
    "                self.w_hidden_output -= self.lr * d_output * hidden_state_new\n",
    "                self.b_output -= self.lr * d_output\n",
    "\n",
    "                self.w_input_hidden -= self.lr * d_hidden * x\n",
    "                self.w_hidden_hidden -= self.lr * d_hidden * hidden_state\n",
    "                self.b_hidden -= self.lr * d_hidden\n",
    "\n",
    "                hidden_state = hidden_state_new\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {total_loss:.4f}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        hidden_state = 0\n",
    "        outputs = []\n",
    "        for x in inputs:\n",
    "            hidden_input = self.w_input_hidden * x + self.w_hidden_hidden * hidden_state + self.b_hidden\n",
    "            hidden_state = sigmoid(hidden_input)\n",
    "\n",
    "            output_input = self.w_hidden_output * hidden_state + self.b_output\n",
    "            prediction = sigmoid(output_input)\n",
    "            outputs.append(round(prediction))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789a4d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 9.7750\n",
      "Epoch 10: Loss = 9.0703\n",
      "Epoch 20: Loss = 7.5908\n",
      "Epoch 30: Loss = 5.2404\n",
      "Epoch 40: Loss = 3.2729\n",
      "Epoch 50: Loss = 2.1369\n",
      "Epoch 60: Loss = 1.5132\n",
      "Epoch 70: Loss = 1.1457\n",
      "Epoch 80: Loss = 0.9109\n",
      "Epoch 90: Loss = 0.7503\n",
      "Epoch 100: Loss = 0.6346\n",
      "Epoch 110: Loss = 0.5477\n",
      "Epoch 120: Loss = 0.4803\n",
      "Epoch 130: Loss = 0.4266\n",
      "Epoch 140: Loss = 0.3830\n",
      "Epoch 150: Loss = 0.3470\n",
      "Epoch 160: Loss = 0.3167\n",
      "Epoch 170: Loss = 0.2909\n",
      "Epoch 180: Loss = 0.2687\n",
      "Epoch 190: Loss = 0.2495\n",
      "\n",
      "Predictions:\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n",
      "Input: 0 → Predicted: 1, Actual: 1\n",
      "Input: 1 → Predicted: 0, Actual: 0\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(epochs=200)\n",
    "rnn.train(inputs, targets)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "predictions = rnn.predict(inputs)\n",
    "for i in range(20):\n",
    "    print(f\"Input: {inputs[i]} → Predicted: {predictions[i]}, Actual: {targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07ac76",
   "metadata": {},
   "source": [
    "## What We’ve Understood from RNN\n",
    "\n",
    "Recurrent Neural Networks are designed for sequence-based problems — anything where the order matters. Unlike normal networks, RNNs maintain a small memory of previous steps (called the **hidden state**) and update it as they move through the sequence.\n",
    "\n",
    "Here’s what we saw:\n",
    "- The RNN processed one input at a time, remembering what came before using the hidden state.\n",
    "- Each prediction depended not just on the current input but on what the RNN had “seen” earlier.\n",
    "- Backpropagation in RNNs is slightly different. It flows through time, so the error at time `t` affects earlier weights — we only did 1-step BPTT to keep things simple.\n",
    "- Our model learned the XOR-style alternating pattern — a simple but perfect demo of how RNNs can “remember” something beyond a single input.\n",
    "\n",
    "This scratch implementation gave us a clear view of what’s happening inside — no magic libraries, just real logic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
